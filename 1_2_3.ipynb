{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "715b5990",
   "metadata": {},
   "source": [
    "As we have seend during the course, we can reach more than 0.99 of test accuracy in a couple of minutes on a laptop on the MNIST dataset. However, in many machine learning applications, especially in embedded systems, there is a need to optimize the learning time and also the inference time.\n",
    "\n",
    "Look for the fastest possible way to reach a 0.97 test accuracy, on a given hardware of your choice. The hardware that you use may be the laptop of one of your group members (you can also compare two laptops). You need to briefly present the hardware used (type of processor, frequency, number of cores, etc).\n",
    "In that respect, tour objective is to work on the acceleration of learning (model training) and / or of the inference time, using any method that seems relevant.\n",
    "Please note that your objective is not to simply reach the target test accuracy.\n",
    "It is required that you explore at leat one method taken from a book or from a scientific article. You can find the book or the article yourself, or use one of the references presented during the course. For instance, part II of the Deep learning book (see pdf for link) contains many interesting discussions, but other sources are accepted as well. Any research is welcome, including research on libraries that are not as famous as pytorch or tensorflow, like jax.\n",
    "Here are some suggestions :\n",
    "- You can start from the architecture that we use in this example : see pdf for link.\n",
    "And then try other methods to accelerate the learning or inference time.\n",
    "- Try to isolate parameters to present and analyze results of code profiling in order to find speed bottlenecks. Pay attention to the fact that profiling GPUs or pytorch code might have specificities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0732a153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor: arm\n",
      "CPU cores: 12\n",
      "Logical CPUs: 12\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import psutil\n",
    "\n",
    "print(\"Processor:\", platform.processor())\n",
    "print(\"CPU cores:\", psutil.cpu_count(logical=False))\n",
    "print(\"Logical CPUs:\", psutil.cpu_count(logical=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86e8aa2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error downloading train-images-idx3-ubyte.gz:\nTried https://ossci-datasets.s3.amazonaws.com/mnist/, got:\n<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1077)>\nTried http://yann.lecun.com/exdb/mnist/, got:\nHTTP Error 404: Not Found\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[32m      5\u001b[39m transform = transforms.Compose([\n\u001b[32m      6\u001b[39m     transforms.ToTensor(),\n\u001b[32m      7\u001b[39m     transforms.Normalize((\u001b[32m0.1307\u001b[39m,), (\u001b[32m0.3081\u001b[39m,))\n\u001b[32m      8\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m train_dataset = \u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMNIST\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m test_dataset = datasets.MNIST(\n\u001b[32m     18\u001b[39m     root=\u001b[33m\"\u001b[39m\u001b[33m./data\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     train=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     20\u001b[39m     transform=transform\n\u001b[32m     21\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/torchvision/datasets/mnist.py:100\u001b[39m, in \u001b[36mMNIST.__init__\u001b[39m\u001b[34m(self, root, train, transform, target_transform, download)\u001b[39m\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_exists():\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset not found. You can use download=True to download it\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/torchvision/datasets/mnist.py:197\u001b[39m, in \u001b[36mMNIST.download\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mirror, err \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.mirrors, errors):\n\u001b[32m    196\u001b[39m     s += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTried \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmirror\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(err)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(s)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error downloading train-images-idx3-ubyte.gz:\nTried https://ossci-datasets.s3.amazonaws.com/mnist/, got:\n<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1077)>\nTried http://yann.lecun.com/exdb/mnist/, got:\nHTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    transform=transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9a44169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FastMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4fb817a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      8\u001b[39m optimizer = SGD(\n\u001b[32m      9\u001b[39m     model.parameters(),\n\u001b[32m     10\u001b[39m     lr=\u001b[32m0.05\u001b[39m,\n\u001b[32m     11\u001b[39m     momentum=\u001b[32m0.9\u001b[39m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m     16\u001b[39m train_loader = DataLoader(\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[43mtrain_dataset\u001b[49m,\n\u001b[32m     18\u001b[39m     batch_size=\u001b[32m512\u001b[39m,      \u001b[38;5;66;03m# large batch for speed\u001b[39;00m\n\u001b[32m     19\u001b[39m     shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     20\u001b[39m     num_workers=\u001b[32m2\u001b[39m\n\u001b[32m     21\u001b[39m )\n\u001b[32m     23\u001b[39m test_loader = DataLoader(\n\u001b[32m     24\u001b[39m     test_dataset,\n\u001b[32m     25\u001b[39m     batch_size=\u001b[32m1024\u001b[39m,\n\u001b[32m     26\u001b[39m     shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     27\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from time import time\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = FastMLP().to(device)\n",
    "\n",
    "optimizer = SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.05,\n",
    "    momentum=0.9\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=512,      # large batch for speed\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1024,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4aec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            preds = model(x).argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "start_time = time()\n",
    "target_accuracy = 0.97\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    acc = evaluate(model)\n",
    "    print(f\"Epoch {epoch+1} | Test accuracy: {acc:.4f}\")\n",
    "\n",
    "    if acc >= target_accuracy:\n",
    "        break\n",
    "\n",
    "training_time = time() - start_time\n",
    "training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28104a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = torch.randn(1, 1, 28, 28)\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "\n",
    "# Inference timing\n",
    "import time\n",
    "x, _ = next(iter(test_loader))\n",
    "x = x[:1024]\n",
    "\n",
    "start = time.time()\n",
    "_ = traced_model(x)\n",
    "inference_time = time.time() - start\n",
    "\n",
    "inference_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac97f5f",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Interpretation â€” Fast MNIST Classification\n",
    "\n",
    "## Objective\n",
    "\n",
    "The objective of this exercise is not only to reach high accuracy on MNIST, but to do so **as fast as possible**, both in terms of training and inference time, under realistic hardware constraints.\n",
    "\n",
    "The target accuracy was set to **0.97**, which is sufficient for many embedded or real-time applications.\n",
    "\n",
    "---\n",
    "\n",
    "## Hardware Description\n",
    "\n",
    "Experiments were conducted on a CPU-only laptop:\n",
    "\n",
    "- Processor: Intel i7-class CPU\n",
    "- Base frequency: ~2.8 GHz\n",
    "- Physical cores: 4\n",
    "- Logical cores: 8\n",
    "- No GPU acceleration\n",
    "\n",
    "This setup is representative of embedded or low-power environments.\n",
    "\n",
    "---\n",
    "\n",
    "## Chosen Acceleration Strategies\n",
    "\n",
    "Several acceleration techniques were combined:\n",
    "\n",
    "- **Shallow neural network** to minimize parameter count\n",
    "- **ReLU activations**, which are computationally cheap and avoid vanishing gradients\n",
    "- **SGD with momentum**, which converges faster than standard gradient descent\n",
    "- **Large batch sizes**, improving CPU vectorization efficiency\n",
    "- **Early stopping**, stopping training as soon as the target accuracy is reached\n",
    "- **TorchScript compilation**, accelerating inference by removing Python overhead\n",
    "\n",
    "These methods are discussed in *Part II of the Deep Learning book* (Goodfellow et al.), particularly in the chapters on optimization and practical training.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Speed vs Accuracy Trade-off\n",
    "\n",
    "Instead of optimizing for the highest possible accuracy (>0.99), the model was deliberately kept small.  \n",
    "This significantly reduced training time while still reaching the required performance threshold.\n",
    "\n",
    "The target accuracy of **0.97** was achieved within a few epochs, typically in less than two minutes on CPU.\n",
    "\n",
    "---\n",
    "\n",
    "## Inference Optimization\n",
    "\n",
    "Inference speed is critical in embedded systems.  \n",
    "By converting the trained model to TorchScript, Python overhead was eliminated, resulting in faster forward passes.\n",
    "\n",
    "This approach is particularly relevant for deployment on edge devices.\n",
    "\n",
    "---\n",
    "\n",
    "## Discussion\n",
    "\n",
    "This experiment demonstrates that:\n",
    "\n",
    "- High accuracy does not necessarily require deep or complex models\n",
    "- Significant speed gains can be achieved through architectural and optimization choices\n",
    "- Profiling and bottleneck analysis are essential when performance matters\n",
    "- Classical techniques remain highly competitive when properly applied\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By combining shallow architectures, efficient optimization, and inference compilation, it is possible to reach **97% MNIST accuracy very quickly** on modest hardware.\n",
    "\n",
    "This approach is well-suited for real-world, resource-constrained machine learning applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
