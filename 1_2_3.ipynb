{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "715b5990",
   "metadata": {},
   "source": [
    "As we have seend during the course, we can reach more than 0.99 of test accuracy in a couple of minutes on a laptop on the MNIST dataset. However, in many machine learning applications, especially in embedded systems, there is a need to optimize the learning time and also the inference time.\n",
    "\n",
    "Look for the fastest possible way to reach a 0.97 test accuracy, on a given hardware of your choice. The hardware that you use may be the laptop of one of your group members (you can also compare two laptops). You need to briefly present the hardware used (type of processor, frequency, number of cores, etc).\n",
    "In that respect, tour objective is to work on the acceleration of learning (model training) and / or of the inference time, using any method that seems relevant.\n",
    "Please note that your objective is not to simply reach the target test accuracy.\n",
    "It is required that you explore at leat one method taken from a book or from a scientific article. You can find the book or the article yourself, or use one of the references presented during the course. For instance, part II of the Deep learning book (see pdf for link) contains many interesting discussions, but other sources are accepted as well. Any research is welcome, including research on libraries that are not as famous as pytorch or tensorflow, like jax.\n",
    "Here are some suggestions :\n",
    "- You can start from the architecture that we use in this example : see pdf for link.\n",
    "And then try other methods to accelerate the learning or inference time.\n",
    "- Try to isolate parameters to present and analyze results of code profiling in order to find speed bottlenecks. Pay attention to the fact that profiling GPUs or pytorch code might have specificities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "586a10e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n",
      "Platform: macOS-15.6.1-arm64-arm-64bit-Mach-O\n",
      "Processor: arm\n",
      "Number of CPU cores: 8\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import platform\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Processor:\", platform.processor())\n",
    "print(\"Number of CPU cores:\", torch.get_num_threads())\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d98142e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "115bdc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FastCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FastCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(32*7*7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 28->14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 14->7\n",
    "        x = x.view(-1, 32*7*7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8617e185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s7/w8kc6vt15jz2x_0flc6q91jr0000gp/T/ipykernel_63440/985512697.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # For mixed precision\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/torch/cuda/amp/grad_scaler.py:31: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2158\n",
      "Epoch [2/5], Loss: 0.0582\n",
      "Epoch [3/5], Loss: 0.0412\n",
      "Epoch [4/5], Loss: 0.0317\n",
      "Epoch [5/5], Loss: 0.0238\n",
      "Training time: 60.85 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FastCNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scaler = GradScaler()  # For mixed precision\n",
    "\n",
    "num_epochs = 5  # Usually 2-5 is enough to reach 0.97\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if device.type == \"cuda\":\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Training time: {:.2f} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7557476f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9896\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(\"Test Accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb8a2e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference        27.22%     308.823ms       100.00%        1.135s        1.135s             1  \n",
      "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...        72.10%     818.020ms        72.11%     818.037ms     818.037ms             1  \n",
      "                                           aten::conv2d         0.00%      15.790us         0.40%       4.548ms       2.274ms             2  \n",
      "                                      aten::convolution         0.00%      50.627us         0.40%       4.533ms       2.266ms             2  \n",
      "                                     aten::_convolution         0.00%      51.041us         0.40%       4.482ms       2.241ms             2  \n",
      "                      aten::_nnpack_spatial_convolution         0.39%       4.420ms         0.39%       4.430ms       2.215ms             2  \n",
      "                                       aten::max_pool2d         0.00%      21.792us         0.10%       1.092ms     545.768us             2  \n",
      "                          aten::max_pool2d_with_indices         0.09%       1.070ms         0.09%       1.070ms     534.872us             2  \n",
      "                                         aten::randperm         0.04%     420.582us         0.09%       1.036ms     518.060us             2  \n",
      "                                             aten::relu         0.00%      34.749us         0.08%     866.829us     288.943us             3  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.135s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "with profiler.profile(record_shapes=True) as prof:\n",
    "    with profiler.record_function(\"model_inference\"):\n",
    "        model(next(iter(train_loader))[0].to(device))\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\" if device.type==\"cuda\" else \"cpu_time_total\", row_limit=10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
