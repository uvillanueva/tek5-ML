{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81940d3e",
   "metadata": {},
   "source": [
    "Number of samples : 17570\n",
    "\n",
    "Number of features : 5\n",
    "\n",
    "Target : total_sales(mil) = total sales in million\n",
    "\n",
    "Description : dataset of video game with metadata (console, genre, publisher, developer, release date), a critical score and sales(total and regional).\n",
    "\n",
    "Problème : predict total_sales(mil) in fonction of features (console/genre/publisher/developer/release_year),\n",
    "without using sales regional to avoid leak of information.\n",
    "(we remove critical score because there is missing 90% of data and this is unusable)\n",
    "\n",
    "Intérêt industrie : estimation of comercial potential / understanding factors associated with sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42781d4c",
   "metadata": {},
   "source": [
    "To start we load the csv with all data information for the video games to read it and get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc905b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64016, 12)\n",
      "Index(['title', 'console', 'genre', 'publisher', 'developer', 'critic_score',\n",
      "       ' total_sales(mil) ', ' na_sales(mil) ', ' jp_sales(mil) ',\n",
      "       ' pal_sales(mil) ', ' other_sales(mil) ', 'release_date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"regression_datasets/VideoGames_Sales.csv\", sep=\";\")\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65d433",
   "metadata": {},
   "source": [
    "after we need to remove unecessary column like the title for avoid learn title by heart instead of tentency, sales to avoid leak of information.\n",
    "Then we applies a log function to applied symetry of sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "433ead41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sales_in_number(df):\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    sales_columns = [\n",
    "        \"na_sales(mil)\",\n",
    "        \"jp_sales(mil)\",\n",
    "        \"pal_sales(mil)\",\n",
    "        \"other_sales(mil)\",\n",
    "        \"total_sales(mil)\",\n",
    "    ]\n",
    "\n",
    "    for c in sales_columns:\n",
    "        if c in df.columns:\n",
    "            s = df[c].astype(str).str.strip()\n",
    "            s = s.str.replace(\"\\u00A0\", \"\", regex=False)\n",
    "            s = s.str.replace(r\"[^0-9\\-\\.,]\", \"\", regex=True)\n",
    "            s = s.str.replace(\",\", \".\", regex=False)\n",
    "\n",
    "            df[c] = pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e7f84",
   "metadata": {},
   "source": [
    "We create a function to convert data of sales into number because there are in string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8fe1d95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN target after conversion: 46446 out of 64016\n",
      "Samples: 17570\n",
      "X shape: (17570, 5)\n",
      "y shape: (17570,)\n",
      "y NaN: 0\n",
      "Samples: 17570\n",
      "Raw features: 5\n",
      "Target: total_sales(mil) (log1p transform applied)\n",
      "Missing values per column:\n",
      " console          0\n",
      "genre            0\n",
      "publisher        0\n",
      "developer        2\n",
      "release_year    62\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "df = convert_sales_in_number(df)\n",
    "\n",
    "target = \"total_sales(mil)\"\n",
    "\n",
    "print(\"NaN target after conversion:\", df[target].isna().sum(), \"out of\", len(df))\n",
    "\n",
    "df = df.dropna(subset=[target])\n",
    "df = df[df[target] >= 0]\n",
    "\n",
    "feature_cols = [\"console\", \"genre\", \"publisher\", \"developer\", \"release_date\"]\n",
    "X = df[feature_cols].copy()\n",
    "\n",
    "X[\"release_date\"] = pd.to_datetime(X[\"release_date\"], errors=\"coerce\", dayfirst=True)\n",
    "X[\"release_year\"] = X[\"release_date\"].dt.year\n",
    "X = X.drop(columns=[\"release_date\"])\n",
    "\n",
    "y = np.log1p(df[target])\n",
    "\n",
    "print(\"Samples:\", len(df))\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"y NaN:\", np.isnan(y).sum())\n",
    "\n",
    "print(\"Samples:\", len(df))\n",
    "print(\"Raw features:\", X.shape[1])\n",
    "print(\"Target:\", target, \"(log1p transform applied)\")\n",
    "print(\"Missing values per column:\\n\", X.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95accda3",
   "metadata": {},
   "source": [
    "The csv file contains 64016 entries but the target variable total_sales(mil) is missing for 46446 rows. After cleaning (converting sales to numeric and removing missing targets), 17570 usable sample remain for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978694d",
   "metadata": {},
   "source": [
    "We implement ridge regression with descent of gradiant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeGD:\n",
    "    def __init__(self, lr=1e-3, epochs=3000, l2=1e-3, clip=1.0, verbose=200):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.l2 = l2\n",
    "        self.clip = clip\n",
    "        self.verbose = verbose\n",
    "        self.w = None\n",
    "        self.b = 0.0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n, d = X.shape\n",
    "        self.w = np.zeros(d, dtype=np.float64)\n",
    "        self.b = 0.0\n",
    "\n",
    "        for t in range(1, self.epochs + 1):\n",
    "            yhat = X @ self.w + self.b\n",
    "            err = yhat - y\n",
    "\n",
    "            grad_w = (2.0/n) * (X.T @ err) + 2.0 * self.l2 * self.w\n",
    "            grad_b = (2.0/n) * np.sum(err)\n",
    "\n",
    "            if self.clip is not None:\n",
    "                grad_w = np.clip(grad_w, -self.clip, self.clip)\n",
    "                grad_b = float(np.clip(grad_b, -self.clip, self.clip))\n",
    "\n",
    "            self.w -= self.lr * grad_w\n",
    "            self.b -= self.lr * grad_b\n",
    "\n",
    "            if self.verbose and t % self.verbose == 0:\n",
    "                loss = np.mean(err**2) + self.l2 * np.sum(self.w**2)\n",
    "                if not np.isfinite(loss):\n",
    "                    print(\"Loss became non-finite (diverged). Try smaller lr.\")\n",
    "                    break\n",
    "                print(f\"epoch {t} | loss {loss:.6f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cfb4f4",
   "metadata": {},
   "source": [
    "Now we prepare data, separate train/test and evaluate the regression modele.\n",
    "First we encode one-hot the categorical variables and implement missing values.\n",
    "then we convert in array and separate train/test randomly\n",
    "after define metrics we use a baseline \"Dummy\" to predict mean\n",
    "and finaly we train and predict our model.\n",
    "we need to return to the original sales scale (reverse log1p) and we analyse errors by sales ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b331532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUMMY | MAE(log): 0.21969028843267657 | RMSE(log): 0.32092038760906716 | R2: -1.197554618825869e-05\n",
      "epoch 200 | loss 0.123503\n",
      "epoch 400 | loss 0.107607\n",
      "epoch 600 | loss 0.100989\n",
      "epoch 800 | loss 0.097982\n",
      "epoch 1000 | loss 0.096397\n",
      "epoch 1200 | loss 0.095386\n",
      "epoch 1400 | loss 0.094621\n",
      "epoch 1600 | loss 0.093971\n",
      "epoch 1800 | loss 0.093386\n",
      "epoch 2000 | loss 0.092845\n",
      "epoch 2200 | loss 0.092337\n",
      "epoch 2400 | loss 0.091858\n",
      "epoch 2600 | loss 0.091405\n",
      "epoch 2800 | loss 0.090975\n",
      "epoch 3000 | loss 0.090566\n",
      "RIDGE_GD | MAE(log): 0.2020958216879382 | RMSE(log): 0.30247484564477883 | R2: 0.11163976096675698\n",
      "Original | MAE: 0.3303186862558893 million | RMSE: 0.7981263602284683 million\n",
      "<0.1 count= 1421 MAE(mil)= 0.20541164215922808\n",
      "0.1-0.5 count= 1411 MAE(mil)= 0.11274943212693145\n",
      "0.5-1 count= 390 MAE(mil)= 0.38118609446358626\n",
      "1-5 count= 270 MAE(mil)= 1.4969100044779777\n",
      "5-20 count= 22 MAE(mil)= 7.133281509726325\n"
     ]
    }
   ],
   "source": [
    "X_oh = pd.get_dummies(X, columns=[\"console\",\"genre\",\"publisher\",\"developer\"], dummy_na=False)\n",
    "\n",
    "X_oh[\"release_year\"] = X_oh[\"release_year\"].fillna(X_oh[\"release_year\"].median())\n",
    "\n",
    "mu = X_oh[\"release_year\"].mean()\n",
    "sigma = X_oh[\"release_year\"].std()\n",
    "X_oh[\"release_year\"] = (X_oh[\"release_year\"] - mu) / (sigma + 1e-8)\n",
    "\n",
    "X_mat = X_oh.to_numpy(dtype=np.float64)\n",
    "y_vec = np.asarray(y, dtype=np.float64)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "idx = np.arange(len(y_vec))\n",
    "rng.shuffle(idx)\n",
    "\n",
    "split = int(0.8 * len(idx))\n",
    "train_idx, test_idx = idx[:split], idx[split:]\n",
    "\n",
    "X_train, X_test = X_mat[train_idx], X_mat[test_idx]\n",
    "y_train, y_test = y_vec[train_idx], y_vec[test_idx]\n",
    "\n",
    "def mae(y, yhat):\n",
    "    return np.mean(np.abs(y - yhat))\n",
    "\n",
    "def rmse(y, yhat):\n",
    "    return np.sqrt(np.mean((y - yhat) ** 2))\n",
    "\n",
    "def r2(y, yhat):\n",
    "    ss_res = np.sum((y - yhat)**2)\n",
    "    ss_tot = np.sum((y - np.mean(y))**2)\n",
    "    return 1 - ss_res/ss_tot\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "dummy_pred = np.full_like(y_test, y_mean)\n",
    "\n",
    "print(\"DUMMY | MAE(log):\", mae(y_test, dummy_pred),\n",
    "      \"| RMSE(log):\", rmse(y_test, dummy_pred),\n",
    "      \"| R2:\", r2(y_test, dummy_pred))\n",
    "\n",
    "model = RidgeGD(lr=1e-3, epochs=3000, l2=1e-3, clip=1.0, verbose=200).fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(\"RIDGE_GD | MAE(log):\", mae(y_test, pred),\n",
    "      \"| RMSE(log):\", rmse(y_test, pred),\n",
    "      \"| R2:\", r2(y_test, pred))\n",
    "\n",
    "y_true_m = np.expm1(y_test)\n",
    "y_pred_m = np.expm1(pred)\n",
    "\n",
    "print(\"Original | MAE:\", mae(y_true_m, y_pred_m), \"million\",\n",
    "      \"| RMSE:\", rmse(y_true_m, y_pred_m), \"million\")\n",
    "\n",
    "abs_err = np.abs(y_pred_m - y_true_m)\n",
    "\n",
    "bins = [0, 0.1, 0.5, 1, 5, 20]\n",
    "labels = [\"<0.1\", \"0.1-0.5\", \"0.5-1\", \"1-5\", \"5-20\"]\n",
    "\n",
    "groups = pd.cut(y_true_m, bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "for lab in labels:\n",
    "    mask = (groups == lab)\n",
    "    if mask.sum() > 0:\n",
    "        print(lab, \"count=\", int(mask.sum()), \"MAE(mil)=\", float(abs_err[mask].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d8b1c",
   "metadata": {},
   "source": [
    "We can see that we have the loss who decrease reguliary showing convergence. The model exceeds the baseline (R²≈0.11 vs ≈0), but remains limited by its linear nature and the high variability of sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf1dff7",
   "metadata": {},
   "source": [
    "here We research the hyperparametre (tuning) for the model and  train the final model with the best found hyperparametre.\n",
    "Creation of train and score function with hyperparametre and 600 epoch and return MAE score.\n",
    "Then we make a loop with hyperparametre grid and see the score.\n",
    "After found the best result we make the final training with best hyperparametre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b4a75366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.0001 l2 0.0 MAE(log) 0.22039378571476614\n",
      "lr 0.0005 l2 0.0 MAE(log) 0.1911682945010726\n",
      "lr 0.001 l2 0.0 MAE(log) 0.19474913648057812\n",
      "lr 0.0001 l2 0.0001 MAE(log) 0.22039380949023016\n",
      "lr 0.0005 l2 0.0001 MAE(log) 0.19116844407402192\n",
      "lr 0.001 l2 0.0001 MAE(log) 0.19474920313028418\n",
      "lr 0.0001 l2 0.001 MAE(log) 0.22039402346076287\n",
      "lr 0.0005 l2 0.001 MAE(log) 0.1911697899645993\n",
      "lr 0.001 l2 0.001 MAE(log) 0.19474980299251266\n",
      "lr 0.0001 l2 0.01 MAE(log) 0.22039616231064285\n",
      "lr 0.0005 l2 0.01 MAE(log) 0.1911832225787399\n",
      "lr 0.001 l2 0.01 MAE(log) 0.19475583539129487\n",
      "Best quick: (np.float64(0.1911682945010726), 0.0005, 0.0)\n",
      "epoch 200 | loss 0.138790\n",
      "epoch 400 | loss 0.123454\n",
      "epoch 600 | loss 0.113763\n",
      "epoch 800 | loss 0.107585\n",
      "epoch 1000 | loss 0.103597\n",
      "epoch 1200 | loss 0.100973\n",
      "epoch 1400 | loss 0.099202\n",
      "epoch 1600 | loss 0.097965\n",
      "epoch 1800 | loss 0.097064\n",
      "epoch 2000 | loss 0.096377\n",
      "epoch 2200 | loss 0.095825\n",
      "epoch 2400 | loss 0.095362\n",
      "epoch 2600 | loss 0.094958\n",
      "epoch 2800 | loss 0.094593\n",
      "epoch 3000 | loss 0.094256\n",
      "Final MAE(log): 0.20515794931922993\n"
     ]
    }
   ],
   "source": [
    "def train_and_score(lr, l2, epochs=600):\n",
    "    m = RidgeGD(lr=lr, epochs=epochs, l2=l2, clip=1.0, verbose=0).fit(X_train, y_train)\n",
    "    p = m.predict(X_test)\n",
    "    return mae(y_test, p)\n",
    "\n",
    "best = None\n",
    "for l2 in [0.0, 1e-4, 1e-3, 1e-2]:\n",
    "    for lr in [1e-4, 5e-4, 1e-3]:\n",
    "        score = train_and_score(lr, l2, epochs=600)\n",
    "        print(\"lr\", lr, \"l2\", l2, \"MAE(log)\", score)\n",
    "        if best is None or score < best[0]:\n",
    "            best = (score, lr, l2)\n",
    "\n",
    "print(\"Best quick:\", best)\n",
    "\n",
    "_, best_lr, best_l2 = best\n",
    "model = RidgeGD(lr=best_lr, epochs=3000, l2=best_l2, clip=1.0, verbose=200).fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "print(\"Final MAE(log):\", mae(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf70c7c",
   "metadata": {},
   "source": [
    "We can see that l2 doesn't improve MAE, the best result is l2 = 0. that mean that for this split and these feature, the regularization doesn't provide a net gain.\n",
    "so a pre-turning (600 epoch) on a grid of hyperparametre indicates that lr=5e-4 and l2=0 minimize MAE and are the best hyperparametre. the Long training (3000 epoch) converge with decreasing loss and obtain MAE≈0.205.\n",
    "So we see that the linear model tends toward an average estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf56e894",
   "metadata": {},
   "source": [
    "Here we return to original scales and calculate the absolute error by sample. \n",
    "After we select 10 example randomly for the test and print the prediction for these. \n",
    "Then we select 10 worst error and print these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0380bc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random examples (general behavior) ---\n",
      "True: 0.31 mil | Pred: 0.40 mil | AbsErr: 0.09 mil\n",
      "True: 8.88 mil | Pred: 0.27 mil | AbsErr: 8.61 mil\n",
      "True: 0.01 mil | Pred: 0.25 mil | AbsErr: 0.24 mil\n",
      "True: 1.22 mil | Pred: 0.33 mil | AbsErr: 0.89 mil\n",
      "True: 0.29 mil | Pred: 0.21 mil | AbsErr: 0.08 mil\n",
      "True: 0.02 mil | Pred: 0.21 mil | AbsErr: 0.19 mil\n",
      "True: 0.14 mil | Pred: 0.20 mil | AbsErr: 0.06 mil\n",
      "True: 0.04 mil | Pred: 0.30 mil | AbsErr: 0.26 mil\n",
      "True: 1.48 mil | Pred: 0.26 mil | AbsErr: 1.22 mil\n",
      "True: 0.02 mil | Pred: 0.29 mil | AbsErr: 0.27 mil\n",
      "\n",
      "--- 10 worst predictions errors (limitations / outliers) ---\n",
      "True: 14.82 mil | Pred: 0.35 mil | AbsErr: 14.47 mil\n",
      "True: 13.53 mil | Pred: 0.36 mil | AbsErr: 13.17 mil\n",
      "True: 10.41 mil | Pred: 0.34 mil | AbsErr: 10.07 mil\n",
      "True: 9.96 mil | Pred: 0.30 mil | AbsErr: 9.66 mil\n",
      "True: 9.32 mil | Pred: 0.28 mil | AbsErr: 9.04 mil\n",
      "True: 8.88 mil | Pred: 0.27 mil | AbsErr: 8.61 mil\n",
      "True: 8.01 mil | Pred: 0.36 mil | AbsErr: 7.65 mil\n",
      "True: 7.26 mil | Pred: 0.30 mil | AbsErr: 6.96 mil\n",
      "True: 7.21 mil | Pred: 0.35 mil | AbsErr: 6.86 mil\n",
      "True: 6.89 mil | Pred: 0.25 mil | AbsErr: 6.64 mil\n"
     ]
    }
   ],
   "source": [
    "y_true_m = np.expm1(y_test)\n",
    "y_pred_m = np.expm1(pred)\n",
    "abs_err = np.abs(y_pred_m - y_true_m)\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "choices = rng.choice(len(y_test), size=10, replace=False)\n",
    "\n",
    "print(\"\\n--- Random examples (general behavior) ---\")\n",
    "for i in choices:\n",
    "    print(f\"True: {y_true_m[i]:.2f} mil | Pred: {y_pred_m[i]:.2f} mil | AbsErr: {abs_err[i]:.2f} mil\")\n",
    "    \n",
    "print(\"\\n--- 10 worst predictions errors (limitations / outliers) ---\")\n",
    "worst = np.argsort(-abs_err)[:10]\n",
    "for i in worst:\n",
    "    print(f\"True: {y_true_m[i]:.2f} mil | Pred: {y_pred_m[i]:.2f} mil | AbsErr: {abs_err[i]:.2f} mil\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d5ce9",
   "metadata": {},
   "source": [
    "We can see the biggest mistake involve very high sales and works better on small/medium sales.\n",
    "The model captures an average estimate but strugglies to predict very big success. The example show a systematic underestimation of high-selling game, because these game depend on unobserved factors (licensing, marketing, and brand awarness) and the linear model tends towards an average estimate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
